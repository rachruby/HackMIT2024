{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "from langchain_iris import IRISVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model. This model's output vectors are of size 384\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with engine.connect() as conn:\n",
    "#     with conn.begin():# Load \n",
    "#         sql = f\"\"\"\n",
    "#                 CREATE TABLE UserReviews3 (\n",
    "#     id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "#     description TEXT,\n",
    "#     video TEXT\n",
    "# )\n",
    "#                 \"\"\"\n",
    "#         result = conn.execute(text(sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    with conn.begin():# Load \n",
    "        sql = f\"\"\"\n",
    "                CREATE TABLE UserReviews4 (\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    description TEXT,\n",
    "    video TEXT,\n",
    "    detail  VECTOR(DOUBLE, 384),\n",
    "    description_vector VECTOR(DOUBLE, 384)\n",
    ")\n",
    "                \"\"\"\n",
    "        result = conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../data/test.mov\"\n",
    "description = \"a video taken at hackMIT for fun\"\n",
    "video_data =  video_path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_embedding =model.encode(description, normalize_embeddings=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/hackMIT_details.txt'}, page_content=\"Our journey in developing Rewind has been a fascinating exploration of cutting-edge technology and complex problem-solving. Rewind is an advanced memory system designed to harness the power of Retrieval-Augmented Generation (RAG) combined with intersystem vector search to enhance information retrieval and memory management.\\n\\nConceptualization and Design\\n\\nThe project began with a clear vision: to create a memory system that seamlessly integrates retrieval mechanisms with generative capabilities, allowing for more accurate and contextually relevant information retrieval. Our primary goal was to build a system that not only stores vast amounts of data but also intelligently retrieves and utilizes this information to enhance decision-making and knowledge retention.\\n\\nTechnical Challenges and Innovations\\n\\nOne of the core challenges was implementing an effective RAG-based approach. This involved integrating a generative model with a retrieval system to improve the quality of responses by leveraging external knowledge sources. The use of intersystem vector search was pivotal in this process. We focused on creating a robust vector search mechanism that could navigate and correlate between different systems, ensuring that Rewind could efficiently access and integrate diverse data sources.\\n\\nTo achieve this, we developed a sophisticated vector representation framework that allowed Rewind to map complex information into a high-dimensional space. This made it possible to perform precise searches and retrieve relevant information across disparate systems. The challenge was to ensure that the vector search was not only accurate but also fast enough to handle real-time queries.\\n\\nImplementation and Integration\\n\\nThe implementation phase involved a series of iterative refinements. We integrated advanced vector search algorithms, optimized for performance and scalability, and fine-tuned the RAG components to work harmoniously with the retrieval system. This required close collaboration between our data scientists, engineers, and machine learning experts.\\n\\nWe also prioritized creating a user-friendly interface that would allow users to interact with the system effortlessly. This involved designing intuitive query mechanisms and ensuring that the output was presented in a clear, actionable format.\\n\\nTesting and Optimization\\n\\nTesting Rewind was an intensive process. We conducted extensive trials to ensure the system's reliability and accuracy. This involved stress-testing the vector search capabilities, evaluating the quality of the generated responses, and ensuring that the system could handle diverse and complex queries. Feedback from these tests was invaluable in making iterative improvements and optimizing performance.\\n\\nImpact and Future Directions\\n\\nThe successful deployment of Rewind has demonstrated the potential of combining RAG with intersystem vector search. The system has shown significant improvements in information retrieval accuracy and user satisfaction. Looking ahead, we are excited about exploring further enhancements, such as integrating additional data sources, improving real-time performance, and expanding the systemâ€™s capabilities to handle even more complex queries.\\n\\nBuilding Rewind has been a rewarding experience, showcasing the power of advanced retrieval and generation techniques. It has opened up new possibilities for how we interact with and utilize information, setting the stage for future innovations in memory systems and intelligent search technologies.\\n\\n\")]\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"../data/hackMIT_details.txt\", encoding='utf-8')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    with conn.begin():# Load \n",
    "        sql = \"\"\"\n",
    "                INSERT INTO UserReviews3 (description, video, detail, description_vector)\n",
    "                VALUES (:description, :video, :detail, :vector)\n",
    "            \"\"\"\n",
    "        conn.execute(\n",
    "            text(sql),\n",
    "            {\"description\": description, \"video\": video_data, \"detail\": docs, \"description_vector\": single_embedding}\n",
    "        )\n",
    "        print(\"Row inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_search = \"earthy and creamy taste\"\n",
    "search_vector = model.encode(description_search, normalize_embeddings=True).tolist() # Convert search phrase into a vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    with conn.begin():\n",
    "        sql = text(\"\"\"\n",
    "            SELECT TOP 1 * FROM UserReviews4\n",
    "            ORDER BY VECTOR_DOT_PRODUCT(description_vector, TO_VECTOR(:search_vector)) DESC\n",
    "        \"\"\")\n",
    "\n",
    "        results = conn.execute(sql, {'search_vector': str(search_vector)}).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=df.columns).iloc[:, :-1] # Remove vector\n",
    "pd.set_option('display.max_colwidth', None)  # Easier to read description\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "COLLECTION_NAME = \"state_of_the_union_test\"\n",
    "\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Number of docs in vector store: {len(db.get()['ids'])}\")\n",
    "query = \"Joint patrols to catch traffickers\"\n",
    "docs_with_score = db.similarity_search_with_score(query)\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
